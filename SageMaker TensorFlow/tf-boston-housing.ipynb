{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Regression Model Workflow with Script Mode and Automatic Model Tuning\n",
    "\n",
    "Starting with TensorFlow version 1.11, you can use SageMaker's prebuilt TensorFlow containers with TensorFlow training scripts similar to those you would use outside SageMaker. This feature is named Script Mode.\n",
    "\n",
    "In this notebook, we will use Script Mode in conjunction with TensorFlow's Eager Execution mode, which is the default execution mode of TensorFlow 2 onwards.  Eager execution is an imperative interface where operations are executed immediately, rather than building a static computational graph. Advantages of Eager Execution include a more intuitive interface with natural Python control flow and less boilerplate, easier debugging, and support for dynamic models and almost all of the available TensorFlow operations. It also features close integration with tf.keras to make rapid prototyping even easier.  \n",
    "\n",
    "To demonstrate Script Mode, this notebook focuses on presenting a relatively complete workflow. The workflow includes local training and hosted training in SageMaker, as well as local inference and SageMaker hosted inference with a real time endpoint. Additionally, Automatic Model Tuning in SageMaker will be used to tune the model's hyperparameters.  This workflow will be applied to a straightforward regression task, predicting house prices based on the well-known Boston Housing dataset. More specifically, this public dataset contains 13 features regarding housing stock of towns in the Boston area, including features such as average number of rooms, accessibility to radial highways, adjacency to the Charles River, etc.  \n",
    "\n",
    "To begin, we'll import some necessary packages and set up directories for training and test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "\n",
    "Next, we'll import the dataset. The dataset itself is small and relatively issue-free. For example, there are no missing values, a common problem for many other datasets. Accordingly, preprocessing just involves normalizing the data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 0us/step\n",
      "x train (404, 13) 2.6029783389231392e-15 0.9999999879626582\n",
      "y train (404,) 22.395049504950492 9.199035423364862\n",
      "x test (102, 13) 0.020826991529340172 0.9836083314719052\n",
      "y test (102,) 23.07843137254902 9.123806690181466\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "mean = x_train.mean(axis=0)\n",
    "std = x_train.std(axis=0)\n",
    "\n",
    "x_train = (x_train - mean) / (std + 1e-8)\n",
    "x_test = (x_test - mean) / (std + 1e-8)\n",
    "\n",
    "print('x train', x_train.shape, x_train.mean(), x_train.std())\n",
    "print('y train', y_train.shape, y_train.mean(), y_train.std())\n",
    "print('x test', x_test.shape, x_test.mean(), x_test.std())\n",
    "print('y test', y_test.shape, y_test.mean(), y_test.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is saved as Numpy files prior to both local mode training and hosted training in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Local Mode training\n",
    "\n",
    "Amazon SageMaker’s Local Mode training feature is a convenient way to make sure your code is working as expected before moving on to full scale, hosted training. To train in Local Mode, it is necessary to have docker-compose or nvidia-docker-compose (for GPU) installed in the notebook instance. Running following script will install docker-compose or nvidia-docker-compose and configure the notebook environment for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker instance route table setup is ok. We are good to go.\r\n",
      "SageMaker instance routing for Docker is ok. We are good to go!\r\n"
     ]
    }
   ],
   "source": [
    "!/bin/bash ./local_mode_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up a TensorFlow Estimator for Local Mode training. One of the key parameters for an Estimator is the `train_instance_type`, which is the kind of hardware on which training will run. In the case of Local Mode, we simply set this parameter to `local` to invoke Local Mode training on the CPU, or to `local_gpu` if the instance has a GPU. Other parameters of note are the algorithm’s hyperparameters, which are passed in as a dictionary, and a Boolean parameter indicating that we are using Script Mode. \n",
    "\n",
    "Recall that we are using Local Mode here mainly to make sure our code is working. Accordingly, instead of performing a full cycle of training with many epochs (passes over the full dataset), we'll train only for a small number of epochs to confirm the code is working properly and avoid wasting training time unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model_dir = '/opt/ml/model'\n",
    "train_instance_type = 'local'\n",
    "hyperparameters = {'epochs': 5, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "local_estimator = TensorFlow(\n",
    "                       source_dir='tf-bostonhousing-script-mode',\n",
    "                       entry_point='train.py',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-scriptmode-bostonhousing',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpy22r65w6_algo-1-p5c6k_1 ... \n",
      "\u001b[1BAttaching to tmpy22r65w6_algo-1-p5c6k_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m 2019-12-05 03:53:26,394 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m 2019-12-05 03:53:26,399 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m 2019-12-05 03:53:26,536 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m 2019-12-05 03:53:26,550 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m 2019-12-05 03:53:26,564 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m 2019-12-05 03:53:26,574 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"current_host\": \"algo-1-p5c6k\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"algo-1-p5c6k\"\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"epochs\": 5,\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"batch_size\": 128,\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"learning_rate\": 0.01,\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"model_dir\": \"/opt/ml/model\"\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"train\": {\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         },\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"test\": {\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"job_name\": \"tf-scriptmode-bostonhousing-2019-12-05-03-53-23-716\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"master_hostname\": \"algo-1-p5c6k\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-149669388636/tf-scriptmode-bostonhousing-2019-12-05-03-53-23-716/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"current_host\": \"algo-1-p5c6k\",\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m             \"algo-1-p5c6k\"\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m         ]\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_HOSTS=[\"algo-1-p5c6k\"]\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_HPS={\"batch_size\":128,\"epochs\":5,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\"}\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-p5c6k\",\"hosts\":[\"algo-1-p5c6k\"]}\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_CURRENT_HOST=algo-1-p5c6k\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-149669388636/tf-scriptmode-bostonhousing-2019-12-05-03-53-23-716/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-p5c6k\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-p5c6k\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":5,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-scriptmode-bostonhousing-2019-12-05-03-53-23-716\",\"log_level\":20,\"master_hostname\":\"algo-1-p5c6k\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-149669388636/tf-scriptmode-bostonhousing-2019-12-05-03-53-23-716/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-p5c6k\",\"hosts\":[\"algo-1-p5c6k\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"5\",\"--learning_rate\",\"0.01\",\"--model_dir\",\"/opt/ml/model\"]\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_HP_EPOCHS=5\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_HP_BATCH_SIZE=128\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_HP_LEARNING_RATE=0.01\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m SM_HP_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m /usr/local/bin/python3.6 train.py --batch_size 128 --epochs 5 --learning_rate 0.01 --model_dir /opt/ml/model\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m \n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m x train (404, 13) y train (404,)\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m x test (102, 13) y test (102,)\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m /cpu:0\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m batch_size = 128, epochs = 5, learning rate = 0.01\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Use tf.cast instead.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Train on 404 samples, validate on 102 samples\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Epoch 1/5\n",
      "404/404 [==============================] - 0s 629us/sample - loss: 539.0984 - val_loss: 424.8362\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Epoch 2/5\n",
      "404/404 [==============================] - 0s 11us/sample - loss: 356.4195 - val_loss: 256.6098\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Epoch 3/5\n",
      "404/404 [==============================] - 0s 10us/sample - loss: 207.4756 - val_loss: 142.9375\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Epoch 4/5\n",
      "404/404 [==============================] - 0s 11us/sample - loss: 119.1653 - val_loss: 92.6083\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Epoch 5/5\n",
      "404/404 [==============================] - 0s 11us/sample - loss: 82.4789 - val_loss: 67.8991\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m  - 0s - loss: 67.8991\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Test MSE : 67.89910888671875\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m WARNING:tensorflow:Saver is deprecated, please switch to tf.train.Checkpoint or tf.keras.Model.save_weights for training checkpoints. When executing eagerly variables do not necessarily have unique names, and so the variable.name-based lookups Saver performs are error-prone.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:257: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m Instructions for updating:\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m WARNING:tensorflow:Export includes no default signature!\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m WARNING:tensorflow:Export includes no default signature!\n",
      "\u001b[36malgo-1-p5c6k_1  |\u001b[0m 2019-12-05 03:53:29,698 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpy22r65w6_algo-1-p5c6k_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "inputs = {'train': f'file://{train_dir}',\n",
    "          'test': f'file://{test_dir}'}\n",
    "\n",
    "local_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Mode endpoint\n",
    "\n",
    "While Amazon SageMaker’s Local Mode training is very useful to make sure your training code is working before moving on to full scale training, it also would be useful to have a convenient way to test your model locally before incurring the time and expense of deploying it to production. One possibility is to fetch the SavedModel artifact or a model checkpoint saved in Amazon S3, and load it in your notebook for testing. We'll explore doing that in another section of this notebook below.  However, an even easier way to do this is to use the Amazon SageMaker SDK to do this work for you.\n",
    "\n",
    "The Estimator object from the Local Mode training job can be used to deploy a model locally with a single line of code. With one exception, this code is the same as the code you would use to deploy to production. In particular, all you need to do is invoke the local Estimator's deploy method, and similarly to Local Mode training, specify the instance type as either `local_gpu` or `local` depending on whether your notebook instance is a GPU instance or CPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmpz9gvosfw_algo-1-7zd3z_1\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m INFO:__main__:starting services\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m INFO:__main__:using default model name: model\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m INFO:__main__:tensorflow serving model config: \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m model_config_list: {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   config: {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     name: \"model\",\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     base_path: \"/opt/ml/model\",\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     model_platform: \"tensorflow\"\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   },\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m INFO:__main__:nginx config: \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m load_module modules/ngx_http_js_module.so;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m worker_processes auto;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m daemon off;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m pid /tmp/nginx.pid;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m error_log  /dev/stderr info;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m worker_rlimit_nofile 4096;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m events {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   worker_connections 2048;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m http {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   include /etc/nginx/mime.types;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   default_type application/json;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   access_log /dev/stdout combined;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   js_include tensorflow-serving.js;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   upstream tfs_upstream {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     server localhost:8501;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   upstream gunicorn_upstream {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     server unix:/tmp/gunicorn.sock fail_timeout=1;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   server {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     listen 8080 deferred;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     client_max_body_size 0;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     client_body_buffer_size 100m;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     subrequest_output_buffer_size 100m;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     set $tfs_version 1.13;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     set $default_tfs_model model;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     location /tfs {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m         rewrite ^/tfs/(.*) /$1  break;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m         proxy_redirect off;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m         proxy_pass_request_headers off;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m         proxy_set_header Content-Type 'application/json';\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m         proxy_set_header Accept 'application/json';\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m         proxy_pass http://tfs_upstream;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     location /ping {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m         js_content ping;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     location /invocations {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m         js_content invocations;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     location / {\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m         return 404 '{\"error\": \"Not Found\"}';\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m     keepalive_timeout 3;\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m   }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m INFO:__main__:tensorflow version info:\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m TensorFlow ModelServer: 1.13.0-rc1+dev.sha.f16e7778\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m TensorFlow Library: 1.13.1\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m INFO:__main__:started tensorflow serving (pid: 8)\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m INFO:__main__:nginx version info:\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m nginx version: nginx/1.16.0\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m built by gcc 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04) \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m built with OpenSSL 1.1.0g  2 Nov 2017 (running with OpenSSL 1.1.1  11 Sep 2018)\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m TLS SNI support enabled\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.16.0/debian/debuild-base/nginx-1.16.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m INFO:__main__:started nginx (pid: 10)\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: using the \"epoll\" event method\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: nginx/1.16.0\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: built by gcc 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04) \r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: OS: Linux 4.14.152-98.182.amzn1.x86_64\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: getrlimit(RLIMIT_NOFILE): 1024:4096\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: start worker processes\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: start worker process 11\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: start worker process 12\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: start worker process 13\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019/12/05 03:55:44 [notice] 10#10: start worker process 14\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.213528: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.213556: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: model\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.313773: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 1575518008}\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.313796: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 1575518008}\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.313811: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 1575518008}\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.313835: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /opt/ml/model/1575518008\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.313849: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/1575518008\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.316670: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.317908: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.318520: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.329894: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.336895: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:132] Running initialization op on SavedModel bundle.\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.338368: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 24514 microseconds.\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.338402: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:101] No warmup data file found at /opt/ml/model/1575518008/assets.extra/tf_serving_warmup_requests\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.338514: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: model version: 1575518008}\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.339879: I tensorflow_serving/model_servers/server.cc:313] Running gRPC ModelServer at 0.0.0.0:9000 ...\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m [evhttp_server.cc : 237] RAW: Entering the event loop ...\r\n",
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 2019-12-05 03:55:44.340654: I tensorflow_serving/model_servers/server.cc:333] Exporting HTTP/REST API at:localhost:8501 ...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\u001b[36malgo-1-7zd3z_1  |\u001b[0m 172.18.0.1 - - [05/Dec/2019:03:55:45 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"-\"\r\n"
     ]
    }
   ],
   "source": [
    "local_predictor = local_estimator.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get predictions from the local endpoint, simply invoke the Predictor's predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-7zd3z_1  |\u001b[0m 172.18.0.1 - - [05/Dec/2019:03:55:45 +0000] \"POST /invocations HTTP/1.1\" 200 137 \"-\" \"-\"\r\n"
     ]
    }
   ],
   "source": [
    "local_results = local_predictor.predict(x_test[:10])['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, the predictions can be compared against the actual target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \t[14.3 20.5 21.4 21.1 21.7 20.1 21.8 21.6 18.7 19.5]\n",
      "target values: \t[ 7.2 18.8 19.  27.  22.2 24.5 31.2 22.9 20.5 23.2]\n"
     ]
    }
   ],
   "source": [
    "local_preds_flat_list = [float('%.1f'%(item)) for sublist in local_results for item in sublist]\n",
    "print('predictions: \\t{}'.format(np.array(local_preds_flat_list)))\n",
    "print('target values: \\t{}'.format(y_test[:10].round(decimals=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only trained the model for a few epochs and there is room for improvement, but the predictions so far should at least appear reasonably within the ballpark.  \n",
    "\n",
    "To avoid having the TensorFlow Serving container running indefinitely on this notebook instance, simply gracefully shut it down by calling the `delete_endpoint` method of the Predictor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SageMaker hosted training\n",
    "\n",
    "Now that we've confirmed our code is working locally, we can move on to use SageMaker's hosted training functionality. Hosted training is preferred to for doing actual training, especially large-scale, distributed training.  Unlike Local Mode training, for hosted training the actual training itself occurs not on the notebook instance, but on a separate cluster of machines managed by SageMaker.  Before starting hosted training, the data must be uploaded to S3. We'll do that now, and confirm the upload was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = 'tf-eager-scriptmode-bostonhousing'\n",
    "\n",
    "traindata_s3_prefix = '{}/data/train'.format(s3_prefix)\n",
    "testdata_s3_prefix = '{}/data/test'.format(s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-east-1-149669388636/tf-eager-scriptmode-bostonhousing/data/train', 'test': 's3://sagemaker-us-east-1-149669388636/tf-eager-scriptmode-bostonhousing/data/test'}\n"
     ]
    }
   ],
   "source": [
    "train_s3 = sagemaker.Session().upload_data(path='./data/train/', key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sagemaker.Session().upload_data(path='./data/test/', key_prefix=testdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'test': test_s3}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to set up an Estimator object for hosted training. It is similar to the Local Mode Estimator, except the `train_instance_type` has been set to a ML instance type instead of `local` for Local Mode. Also, since we know our code is working now, we train for a larger number of epochs.\n",
    "\n",
    "With these two changes, we simply call `fit` to start the actual hosted training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = 'ml.c5.xlarge'\n",
    "hyperparameters = {'epochs': 30, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "                       source_dir='tf-bostonhousing-script-mode',\n",
    "                       entry_point='train.py',\n",
    "                       model_dir=model_dir,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-scriptmode-bostonhousing',\n",
    "                       framework_version='1.13',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-05 03:56:51 Starting - Starting the training job...\n",
      "2019-12-05 03:56:53 Starting - Launching requested ML instances......\n",
      "2019-12-05 03:58:04 Starting - Preparing the instances for training...\n",
      "2019-12-05 03:58:33 Downloading - Downloading input data...\n",
      "2019-12-05 03:59:13 Training - Training image download completed. Training in progress..\u001b[34m2019-12-05 03:59:18,063 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2019-12-05 03:59:18,069 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-12-05 03:59:18,475 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-12-05 03:59:18,501 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-12-05 03:59:18,525 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2019-12-05 03:59:18,536 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"epochs\": 30,\n",
      "        \"learning_rate\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-scriptmode-bostonhousing-2019-12-05-03-56-51-493\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-149669388636/tf-scriptmode-bostonhousing-2019-12-05-03-56-51-493/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":30,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-149669388636/tf-scriptmode-bostonhousing-2019-12-05-03-56-51-493/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":30,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-scriptmode-bostonhousing-2019-12-05-03-56-51-493\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-149669388636/tf-scriptmode-bostonhousing-2019-12-05-03-56-51-493/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"30\",\"--learning_rate\",\"0.01\",\"--model_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=30\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 train.py --batch_size 128 --epochs 30 --learning_rate 0.01 --model_dir /opt/ml/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mx train (404, 13) y train (404,)\u001b[0m\n",
      "\u001b[34mx test (102, 13) y test (102,)\u001b[0m\n",
      "\u001b[34m/cpu:0\u001b[0m\n",
      "\u001b[34mbatch_size = 128, epochs = 30, learning rate = 0.01\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mColocations handled automatically by placer.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.cast instead.\u001b[0m\n",
      "\u001b[34mTrain on 404 samples, validate on 102 samples\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.cast instead.\u001b[0m\n",
      "\u001b[34mEpoch 1/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 590.9095#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 1ms/sample - loss: 539.0984 - val_loss: 424.8362\u001b[0m\n",
      "\u001b[34mEpoch 2/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 381.3707#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 356.4195 - val_loss: 256.6098\u001b[0m\n",
      "\u001b[34mEpoch 3/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 200.7149#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 207.4756 - val_loss: 142.9375\u001b[0m\n",
      "\u001b[34mEpoch 4/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 151.8662#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 10us/sample - loss: 119.1653 - val_loss: 92.6083\u001b[0m\n",
      "\u001b[34mEpoch 5/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 66.6184#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 11us/sample - loss: 82.4789 - val_loss: 67.8991\u001b[0m\n",
      "\u001b[34mEpoch 6/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 60.5760#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 64.3348 - val_loss: 55.8237\u001b[0m\n",
      "\u001b[34mEpoch 7/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 46.9525#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 56.5018 - val_loss: 50.2700\u001b[0m\n",
      "\u001b[34mEpoch 8/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 56.1402#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 15us/sample - loss: 52.1916 - val_loss: 47.1901\u001b[0m\n",
      "\u001b[34mEpoch 9/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 45.5182#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 49.4004 - val_loss: 44.1232\u001b[0m\n",
      "\u001b[34mEpoch 10/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 57.6574#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 46.7730 - val_loss: 41.6631\u001b[0m\n",
      "\u001b[34mEpoch 11/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 50.9717#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 16us/sample - loss: 44.4114 - val_loss: 38.9199\u001b[0m\n",
      "\u001b[34mEpoch 12/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 55.9605#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 11us/sample - loss: 41.8642 - val_loss: 37.7610\u001b[0m\n",
      "\u001b[34mEpoch 13/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 40.7199#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 16us/sample - loss: 40.1955 - val_loss: 36.8901\u001b[0m\n",
      "\u001b[34mEpoch 14/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 35.5389#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 38.4581 - val_loss: 35.0565\u001b[0m\n",
      "\u001b[34mEpoch 15/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 43.5865#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 37.0135 - val_loss: 33.2459\u001b[0m\n",
      "\u001b[34mEpoch 16/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 34.3783#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 14us/sample - loss: 34.9809 - val_loss: 34.8623\u001b[0m\n",
      "\u001b[34mEpoch 17/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 39.1979#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 15us/sample - loss: 33.9279 - val_loss: 30.6030\u001b[0m\n",
      "\u001b[34mEpoch 18/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 37.7452#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 12us/sample - loss: 32.2011 - val_loss: 30.3565\u001b[0m\n",
      "\u001b[34mEpoch 19/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 44.7372#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 17us/sample - loss: 31.1913 - val_loss: 29.2792\u001b[0m\n",
      "\u001b[34mEpoch 20/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 23.3036#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 29.4348 - val_loss: 28.5203\u001b[0m\n",
      "\u001b[34mEpoch 21/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 29.7128#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 16us/sample - loss: 28.1710 - val_loss: 27.7233\u001b[0m\n",
      "\u001b[34mEpoch 22/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 31.5723#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 11us/sample - loss: 27.9131 - val_loss: 26.2798\u001b[0m\n",
      "\u001b[34mEpoch 23/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 26.9570#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 15us/sample - loss: 26.1799 - val_loss: 28.7969\u001b[0m\n",
      "\u001b[34mEpoch 24/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 25.2566#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 16us/sample - loss: 26.3970 - val_loss: 28.8609\u001b[0m\n",
      "\u001b[34mEpoch 25/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 29.7712#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 24.5229 - val_loss: 36.6842\u001b[0m\n",
      "\u001b[34mEpoch 26/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 28.3030#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 13us/sample - loss: 25.3246 - val_loss: 25.5414\u001b[0m\n",
      "\u001b[34mEpoch 27/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 20.4528#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 11us/sample - loss: 22.3435 - val_loss: 24.1203\u001b[0m\n",
      "\u001b[34mEpoch 28/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 23.4513#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 11us/sample - loss: 21.4600 - val_loss: 23.5237\u001b[0m\n",
      "\u001b[34mEpoch 29/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 20.4006#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 11us/sample - loss: 20.8445 - val_loss: 30.4730\u001b[0m\n",
      "\u001b[34mEpoch 30/30\u001b[0m\n",
      "\u001b[34m#015128/404 [========>.....................] - ETA: 0s - loss: 20.3550#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015404/404 [==============================] - 0s 16us/sample - loss: 22.0569 - val_loss: 42.5571\n",
      " - 0s - loss: 42.5571\u001b[0m\n",
      "\u001b[34mTest MSE : 42.5571174621582\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Saver is deprecated, please switch to tf.train.Checkpoint or tf.keras.Model.save_weights for training checkpoints. When executing eagerly variables do not necessarily have unique names, and so the variable.name-based lookups Saver performs are error-prone.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:257: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Export includes no default signature!\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Export includes no default signature!\u001b[0m\n",
      "\u001b[34m2019-12-05 03:59:23,978 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-12-05 03:59:35 Uploading - Uploading generated training model\n",
      "2019-12-05 03:59:35 Completed - Training job completed\n",
      "Training seconds: 62\n",
      "Billable seconds: 62\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Local Mode training, hosted training produces a model checkpoint saved in S3 that we can retrieve and load. We can then make predictions and compare them with the test set.  This also demonstrates the modularity of SageMaker: having trained the model in SageMaker, you can now take the model out of SageMaker and run it anywhere else.  Alternatively, you can deploy the model using SageMaker's hosted endpoints functionality.\n",
    "\n",
    "Now, instead of using a Local Mode endpoint, we'll go through the steps of downloading the model from Amazon S3 and loading a checkpoint.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 25.0 KiB/25.0 KiB (230.5 KiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-east-1-149669388636/tf-scriptmode-bostonhousing-2019-12-05-03-56-51-493/output/model.tar.gz to model/model.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {estimator.model_data} ./model/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.ckpt.index\r\n",
      "weights.ckpt.data-00000-of-00001\r\n",
      "1575518362/\r\n",
      "1575518362/variables/\r\n",
      "1575518362/variables/variables.index\r\n",
      "1575518362/variables/checkpoint\r\n",
      "1575518362/variables/variables.data-00000-of-00001\r\n",
      "1575518362/assets/\r\n",
      "1575518362/assets/saved_model.json\r\n",
      "1575518362/saved_model.pb\r\n",
      "checkpoint\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p train_model\n",
    "!wget -q -P ./train_model https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/tf-eager-script-mode/train_model/model_def.py\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "from train_model import model_def\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "device = '/cpu:0' \n",
    "\n",
    "with tf.device(device):    \n",
    "    model = model_def.get_model()\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.restore('model/weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \t[12.2 18.9 20.  29.7 21.9 20.9 26.1 21.2 19.3 29. ]\n",
      "target values: \t[ 7.2 18.8 19.  27.  22.2 24.5 31.2 22.9 20.5 23.2]\n"
     ]
    }
   ],
   "source": [
    "with tf.device(device):   \n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "print('predictions: \\t{}'.format(predictions[:10].flatten().round(decimals=1)))\n",
    "print('target values: \\t{}'.format(y_test[:10].round(decimals=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SageMaker hosted endpoint\n",
    "\n",
    "After multiple sanity checks, we're confident that our model is performing as expected. If we wish to deploy the model to production, a convenient option is to use a SageMaker hosted endpoint. The endpoint will retrieve the TensorFlow SavedModel created during training and deploy it within a TensorFlow Serving container. This all can be accomplished with one line of code, an invocation of the Estimator's deploy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one last sanity check, we can compare the predictions generated by the endpoint with those generated locally by the model checkpoint we retrieved from hosted training in SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \t[12.2 18.9 20.  29.7 21.9 20.9 26.1 21.2 19.3 29. ]\n",
      "target values: \t[ 7.2 18.8 19.  27.  22.2 24.5 31.2 22.9 20.5 23.2]\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(x_test[:10])['predictions'] \n",
    "flat_list = [float('%.1f'%(item)) for sublist in results for item in sublist]\n",
    "print('predictions: \\t{}'.format(np.array(flat_list)))\n",
    "print('target values: \\t{}'.format(y_test[:10].round(decimals=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the rest of this notebook, you can delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning\n",
    "\n",
    "Selecting the right hyperparameter values to train your model can be difficult. The right answer is dependent on your data; some algorithms have many different hyperparameters that can be tweaked; some are very sensitive to the hyperparameter values selected; and most have a non-linear relationship between model fit and hyperparameter values.  SageMaker Automatic Model Tuning helps automate the hyperparameter tuning process:  it runs multiple training jobs with different hyperparameter combinations to find the set with the best model performance.\n",
    "\n",
    "We begin by specifying the hyperparameters we wish to tune, and the range of values over which to tune each one.  We also must specify an objective metric to be optimized:  in this use case, we'd like to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from time import gmtime, strftime \n",
    "\n",
    "hyperparameter_ranges = {\n",
    "        'learning_rate': ContinuousParameter(0.001, 0.2, scaling_type=\"Logarithmic\"),\n",
    "        'epochs': IntegerParameter(10, 50),\n",
    "        'batch_size': IntegerParameter(64, 256),\n",
    "    }\n",
    "\n",
    "metric_definitions = [{'Name': 'loss',\n",
    "                       'Regex': ' loss: ([0-9\\\\.]+)'},\n",
    "                     {'Name': 'val_loss',\n",
    "                       'Regex': ' val_loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "objective_metric_name = 'val_loss'\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify a HyperparameterTuner object that takes the above definitions as parameters.  Each tuning job must be given a budget - a maximum number of training jobs - and the tuning job will complete once that many training jobs have been executed.  \n",
    "\n",
    "We also can specify how much parallelism to employ, in this case five jobs, meaning that the tuning job will complete after three series of five jobs in parallel have completed.  For the default Bayesian Optimization tuning strategy used here, the search is informed by the results of previous groups of training jobs, so we don't run all of the jobs in parallel, but rather divide the jobs into groups of parallel jobs.  In other words, more parallel jobs will finish tuning sooner, but may sacrifice accuracy. \n",
    "\n",
    "Now we can launch a hyperparameter tuning job by calling the `fit` method of the HyperparameterTuner object. We will wait until the tuning finished, which may take around 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=9,\n",
    "                            max_parallel_jobs=3,\n",
    "                            objective_type=objective_type)\n",
    "\n",
    "tuning_job_name = \"tf-bostonhousing-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "tuner.fit(inputs, job_name=tuning_job_name)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tuning job is finished, we can use the `HyperparameterTuningJobAnalytics` method to list the top 5 tuning jobs with the best performance. Although the results typically vary from tuning job to tuning job, the best validation loss from the tuning job (under the FinalObjectiveValue column) likely will be lower than the validation loss from the hosted training job above.  For an example of a more in-depth analysis of a tuning job, see HPO_Analyze_TuningJob_Results.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.566401</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2019-12-05 04:23:10+00:00</td>\n",
       "      <td>tf-bostonhousing-05-04-13-09-009-6b03c7f8</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2019-12-05 04:22:27+00:00</td>\n",
       "      <td>127.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.028037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.659401</td>\n",
       "      <td>116.0</td>\n",
       "      <td>2019-12-05 04:20:00+00:00</td>\n",
       "      <td>tf-bostonhousing-05-04-13-09-004-7ab66a52</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2019-12-05 04:18:04+00:00</td>\n",
       "      <td>210.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.094824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23.506399</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2019-12-05 04:16:00+00:00</td>\n",
       "      <td>tf-bostonhousing-05-04-13-09-002-65b22106</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2019-12-05 04:15:08+00:00</td>\n",
       "      <td>112.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.075963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.056299</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2019-12-05 04:22:20+00:00</td>\n",
       "      <td>tf-bostonhousing-05-04-13-09-007-4a43c468</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2019-12-05 04:21:11+00:00</td>\n",
       "      <td>251.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.009474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.000999</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2019-12-05 04:19:05+00:00</td>\n",
       "      <td>tf-bostonhousing-05-04-13-09-005-cd9bdf6b</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2019-12-05 04:18:04+00:00</td>\n",
       "      <td>234.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.070810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FinalObjectiveValue  TrainingElapsedTimeSeconds           TrainingEndTime  \\\n",
       "0            19.566401                        43.0 2019-12-05 04:23:10+00:00   \n",
       "5            19.659401                       116.0 2019-12-05 04:20:00+00:00   \n",
       "7            23.506399                        52.0 2019-12-05 04:16:00+00:00   \n",
       "2            26.056299                        69.0 2019-12-05 04:22:20+00:00   \n",
       "4            31.000999                        61.0 2019-12-05 04:19:05+00:00   \n",
       "\n",
       "                             TrainingJobName TrainingJobStatus  \\\n",
       "0  tf-bostonhousing-05-04-13-09-009-6b03c7f8         Completed   \n",
       "5  tf-bostonhousing-05-04-13-09-004-7ab66a52         Completed   \n",
       "7  tf-bostonhousing-05-04-13-09-002-65b22106         Completed   \n",
       "2  tf-bostonhousing-05-04-13-09-007-4a43c468         Completed   \n",
       "4  tf-bostonhousing-05-04-13-09-005-cd9bdf6b         Completed   \n",
       "\n",
       "          TrainingStartTime  batch_size  epochs  learning_rate  \n",
       "0 2019-12-05 04:22:27+00:00       127.0    42.0       0.028037  \n",
       "5 2019-12-05 04:18:04+00:00       210.0    45.0       0.094824  \n",
       "7 2019-12-05 04:15:08+00:00       112.0    10.0       0.075963  \n",
       "2 2019-12-05 04:21:11+00:00       251.0    46.0       0.009474  \n",
       "4 2019-12-05 04:18:04+00:00       234.0    12.0       0.070810  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "tuner_metrics.dataframe().sort_values(['FinalObjectiveValue'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total training time and training jobs status can be checked with the following script. Because automatic early stopping is by default off, all the training jobs should be completed normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total training time is 0.15 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Completed    9\n",
       "Name: TrainingJobStatus, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time = tuner_metrics.dataframe()['TrainingElapsedTimeSeconds'].sum() / 3600\n",
    "print(\"The total training time is {:.2f} hours\".format(total_time))\n",
    "tuner_metrics.dataframe()['TrainingJobStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the best model from the tuning job is better than the model produced by the hosted training job above, we could now easily deploy that model.  By calling the `deploy` method of the HyperparameterTuner object we instantiated above, we can directly deploy the best model from the tuning job to a SageMaker hosted endpoint:\n",
    "\n",
    "`tuning_predictor = tuner.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')`\n",
    "\n",
    "Since we already looked at how to use a SageMaker hosted endpoint above, we won't repeat that here.  We've covered a lot of content in this notebook:  local and hosted training with Script Mode, local and hosted inference in SageMaker, and Automatic Model Tuning.  These are likely to be central elements for most deep learning workflows in SageMaker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
